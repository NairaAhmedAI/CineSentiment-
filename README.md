â€# ğŸ“Œ NLP Project - Logistic Regression with TF-IDF


â€## ğŸ› ï¸ Imports & Preprocessing

â€- **Imported Libraries**:  â€
â€  - `pandas`, `numpy` â†’ Data manipulation and numerical operations  â€
â€  - `matplotlib`, `seaborn` â†’ Data visualization and charts  â€
â€  - `sklearn` â†’ Machine learning (TF-IDF, Logistic Regression, â€evaluation metrics)  â€
â€  - `nltk` â†’ Text preprocessing (stopwords removal, tokenization, â€etc.)  â€

â€- **Preprocessing Steps**:  â€
â€  - Converted text to lowercase  â€
â€  - Removed punctuation, numbers, and irrelevant symbols  â€
â€  - Removed stopwords for cleaner representation  â€
â€  - Applied tokenization for splitting text into words  â€
â€  - Prepared the cleaned data for TF-IDF feature extraction  â€

â€---â€


â€## ğŸ“– Overview
This project applies **text preprocessing** and **machine learning** â€techniques to classify text using **Logistic Regression** with **TF-â€IDF features**.  â€
The workflow includes:â€
â€- Data cleaning and preprocessing  â€
â€- Feature extraction with TF-IDF  â€
â€- Logistic Regression model training  â€
â€- Evaluation using accuracy and confusion matrix  â€
â€- Visualization with WordCloud and bar charts  â€

â€---â€

â€## ğŸ“Š Model Performance

â€- **Accuracy Achieved**: **89.11%** ğŸ¯

â€### âœ… Confusion Matrix (Logistic Regression - TF-IDF)â€

â€|                | Predicted Negative | Predicted Positive |â€
â€|----------------|--------------------|--------------------|â€
â€| **Actual Negative** | **4342** (True Negative) | **619** (False â€Positive) |â€
â€| **Actual Positive** | **470** (False Negative) | **4569** (True â€Positive) |â€

ğŸ”¹ **Interpretation**:  â€
â€- The model correctly predicted **4342 negatives** and **4569 â€positives**.  â€
â€- **619 samples** were misclassified as Positive (False Positives).  â€
â€- **470 samples** were misclassified as Negative (False Negatives).  â€

â€---â€

â€## ğŸ“ˆ Visualizations

â€### â˜ï¸ Word Cloud
The Word Cloud highlights the most frequent words in the dataset after â€preprocessing.  â€
It provides an intuitive view of the **key terms** that dominate the â€corpus, which helps in understanding the text distribution.  â€

â€### ğŸ“Š Bar Charts
The bar charts visualize the **distribution of labels** and other â€dataset characteristics.  â€
They help check for **class balance** and general text statistics.â€

â€---â€

â€## ğŸ” TF-IDF Explanation

â€**TF-IDF** = **Term Frequency â€“ Inverse Document Frequency**  â€

â€- **TF (Term Frequency):** How often a word appears in a document.  â€
â€- **IDF (Inverse Document Frequency):** How unique or rare the word is â€across all documents.  â€
â€- Final TF-IDF score = **TF Ã— IDF**, giving higher weight to â€important, distinguishing words.  â€

ğŸ‘‰ This ensures that common words like *"the"*, *"and"* get **lower â€weight**, while unique, informative words get **higher importance**.â€

â€---â€

â€## â• Custom Threshold & Neutral Class

By default, Logistic Regression predicts only **Positive** or â€â€**Negative**.  â€
A **custom threshold strategy** was added to introduce a **third â€class: Neutral**.  â€

â€### ğŸ”¹ How it works:â€
â€- If the probability for Positive â‰¥ 0.6 â†’ classify as **Positive**  â€
â€- If the probability for Negative â‰¥ 0.6 â†’ classify as **Negative**  â€
â€- Otherwise â†’ classify as **Neutral**  â€

â€### ğŸŒŸ Benefit:â€
This prevents forcing uncertain cases into Positive/Negative and â€instead assigns them as **Neutral**.  â€
It is especially valuable in **Sentiment Analysis**, where some text â€may not strongly express either polarity.â€

â€---â€


âœ… Extract features with TF-IDF Vectorizer

ğŸ“Š Model Training & Results
Model	Accuracy	Key Points
Logistic Regression (TF-IDF)	89.11% ğŸ¯	- Balanced & interpretable
- Robust for text classification
- Confusion Matrix:
â€¢ TN: 4342 â€¢ TP: 4569
â€¢ FP: 619 â€¢ FN: 470
Naive Bayes	85.27%	- Lightweight & fast
- Solid baseline model
- Works best on simpler datasets
Support Vector Machine (SVM)	89.38% â­	- Handles high-dimensional text data
- Strong generalization ability
- Best trade-off for real-world use
Random Forest	86.12%	- Captures non-linear relationships
- Reduces overfitting
- Provides feature importance
ğŸ“Š Model Comparison Chart
Model	Logistic Regression	Naive Bayes	SVM	Random Forest
Accuracy (%)	89.11	85.27	89.38	86.12

ğŸ“Œ SVM achieved the highest accuracy (89.38%) and is the most suitable model for deployment.

ğŸ“ˆ Visualizations

â˜ï¸ WordCloud â†’ Highlights most frequent words

ğŸ“Š Bar Charts â†’ Show dataset distribution & balance

ğŸ“‰ Accuracy Plot â†’ Compare models side by side

âœ… Conclusion

ğŸ”¹ Naive Bayes â†’ Good baseline, fast & lightweight

ğŸ”¹ Logistic Regression â†’ Balanced, robust & interpretable

ğŸ”¹ SVM â†’ Best trade-off between accuracy & generalization â†’ â­ Recommended model

ğŸ”¹ Random Forest â†’ Useful for feature importance & non-linear patterns
