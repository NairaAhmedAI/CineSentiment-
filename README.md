
ğŸ¬ CineSentiment - Sentiment Analysis on IMDB Reviews
ğŸ“– Overview

CineSentiment ğŸ¥ is a Natural Language Processing (NLP) project for sentiment analysis of IMDB movie reviews.
The goal is to classify reviews into Positive, Negative, or Neutral sentiments using text preprocessing, TF-IDF feature extraction, and Machine Learning models.

The project includes:

Data cleaning and preprocessing

Feature extraction with TF-IDF Vectorizer

Training and evaluation of multiple ML models

Visualizations (WordClouds, bar charts, and comparison plots)

ğŸ› ï¸ Tools & Libraries

pandas, numpy â†’ Data handling and numerical operations

matplotlib, seaborn â†’ Data visualization

scikit-learn (sklearn) â†’ TF-IDF, Logistic Regression, Naive Bayes, SVM, Random Forest

nltk â†’ Text preprocessing (stopwords removal, tokenization, etc.)

âš™ï¸ Preprocessing Steps

âœ”ï¸ Convert text to lowercase
âœ”ï¸ Remove punctuation, numbers, and special characters
âœ”ï¸ Remove stopwords
âœ”ï¸ Apply tokenization
âœ”ï¸ Extract features using TF-IDF Vectorizer

ğŸ“Š Model Training & Results

After preprocessing and applying TF-IDF, four machine learning models were trained and compared:

ğŸ”¹ Logistic Regression (TF-IDF)

Accuracy: 89.11% ğŸ¯

Strength: Balanced performance, interpretable, robust for text classification

Confusion Matrix:

True Negatives: 4342

True Positives: 4569

False Positives: 619

False Negatives: 470

ğŸ”¹ Naive Bayes

Accuracy: 85.27%

Strength: Lightweight and fast, provides a good baseline model

Works best on smaller text datasets but less robust on complex text

ğŸ”¹ Support Vector Machine (SVM)

Accuracy: 89.38%

Strength: Handles high-dimensional data effectively, strong generalization ability

Best trade-off between accuracy and generalization â†’ most suitable for real-world use

ğŸ”¹ Random Forest

Accuracy: 86.12%

Strength: Reduces overfitting, captures non-linear relationships

Provides feature importance insights

ğŸ“ˆ Model Comparison
Model	Accuracy	Strength
Logistic Regression	89.11%	Balanced, interpretable
Naive Bayes	85.27%	Lightweight, fast baseline
SVM	89.38%	Best trade-off, strong generalization
Random Forest	86.12%	Non-linear patterns, feature importance

ğŸ“Œ From the results, SVM showed the highest accuracy (89.38%) and strong generalization, making it the best candidate for deployment.

ğŸ“Š Visualizations

WordCloud â†’ Highlights most frequent words in the dataset

Bar Charts â†’ Distribution of classes and text statistics

Accuracy Comparison Plot â†’ Side-by-side performance of models

âœ… Conclusion

Naive Bayes â†’ good baseline

Logistic Regression â†’ robust and interpretable

SVM â†’ best trade-off between accuracy & generalization (most recommended)

Random Forest â†’ useful for interpretability and non-linear patterns
