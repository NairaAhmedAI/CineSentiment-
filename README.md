# CineSentimentüé¨- Sentiment Analysis on IMDB Movie Reviews  

## üìå Overview  
This project focuses on **Sentiment Analysis** of IMDB movie reviews, a classic **Natural Language Processing (NLP)** task.  
The main objective is to classify each review as either **Positive** or **Negative** by building a complete pipeline that covers:  

- Data Collection  
- Text Preprocessing  
- Machine Learning & Deep Learning Models (next steps)  

By leveraging NLP techniques, we aim to transform raw unstructured text into clean and meaningful representations that can be used for predictive modeling.  

---

## üìä Dataset  
We used the **IMDB Dataset of 50K Movie Reviews**, which contains **25,000 positive** and **25,000 negative** reviews.  
This dataset is widely used for benchmarking sentiment classification models.  

üì• Dataset link: [IMDB 50K Movie Reviews - Kaggle](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)  

---

## üõ† Data Preprocessing  

Preprocessing is a critical step in NLP as raw text is often noisy and inconsistent. We applied several transformations to prepare the dataset:  

### 1Ô∏è‚É£ Lowercasing  
All reviews were converted to lowercase to maintain consistency and avoid duplication caused by case sensitivity.  

### 2Ô∏è‚É£ Label Encoding  
The target variable (**sentiment**) was encoded into numeric values:  
- `0` ‚Üí Negative  
- `1` ‚Üí Positive  

### 3Ô∏è‚É£ Cleaning (Regex-based)  
- Removed **emojis** and **hashtags**  
- Removed HTML tags such as `<br>`  
- Normalized text by stripping unnecessary characters  

### 4Ô∏è‚É£ Tokenization & Stopword Removal  
Using **NLTK**, we split sentences into individual tokens (words).  
Stopwords (e.g., *is, the, and*) were removed to reduce noise, but **negation words** (e.g., *not, no*) were kept as they strongly influence sentiment.  

### 5Ô∏è‚É£ Lemmatization with POS Tagging  
- Applied **Part-of-Speech (POS) tagging** to identify word categories (noun, verb, adjective, adverb).  
- Used **WordNet Lemmatizer** from NLTK to reduce words to their base form (e.g., *running ‚Üí run*, *better ‚Üí good*).  

This step ensures that semantically similar words are treated the same way, improving model generalization.  

---

## üîë Why NLTK?  
The **Natural Language Toolkit (NLTK)** is a powerful and widely-used Python library for NLP.  
It provides tools for:  
- Tokenization  
- Stopword filtering  
- POS tagging  
- Lemmatization  
- Corpora and lexical resources (WordNet)  

Its flexibility and ease of use make it an excellent choice for preprocessing in text classification projects like this one.  

# ü§ñ Machine Learning Models

After preprocessing the IMDB dataset, we trained several **classical machine learning models** to classify movie reviews as **positive** or **negative**.  
Each model was trained using **TF-IDF** or **Bag-of-Words** representations, then evaluated on the test set.

---

## üõ†Ô∏è Tools & Libraries
| Category            | Libraries                                                                 |
|---------------------|----------------------------------------------------------------------------|
| Data Handling       | pandas, numpy                                                            |
| Visualization       | matplotlib, seaborn                                                        |
| ML Models           | scikit-learn (Logistic Regression, Naive Bayes, SVM, Random Forest)        |
| NLP Preprocessing   | nltk (tokenization, stopwords removal, lemmatization, POS tagging, etc.)   |

---

## üìä Model Training & Results  

| Model                           | Accuracy  | Key Points                                                                 |
|---------------------------------|-----------|----------------------------------------------------------------------------|
| **Logistic Regression (TF-IDF)** üéØ | **89.11%** | - Balanced & interpretable <br> - Robust for text classification <br> - Implemented a **Custom Threshold = 0.6** instead of the default 0.5. <br> - This adjustment allowed the model to classify reviews into **three categories**: <br> ‚Ä¢ **Positive** (high confidence in positive sentiment) <br> ‚Ä¢ **Negative** (high confidence in negative sentiment) <br> ‚Ä¢ **Neutral** (when neither class exceeds the threshold, capturing uncertain cases) <br> - Confusion Matrix:<br> ‚Ä¢ TN: 4342 ‚Ä¢ TP: 4569 <br> ‚Ä¢ FP: 619 ‚Ä¢ FN: 470 |
| **Naive Bayes**                 | 85.27%    | - Lightweight & fast <br> - Solid baseline model <br> - Works best on simpler datasets |
| **Support Vector Machine (SVM)** ‚≠ê | **89.38%** | - Handles high-dimensional text data <br> - Strong generalization ability <br> - Best trade-off for real-world use |
| **Random Forest**               | 86.12%    | - Captures non-linear relationships <br> - Reduces overfitting <br> - Provides feature importance |


---

## üìä Model Comparison Chart

| Model                | Logistic Regression | Naive Bayes | SVM   | Random Forest |
|----------------------|---------------------|-------------|-------|---------------|
| **Accuracy (%)**     | 89.11               | 85.27       | 89.38 | 86.12         |

üìå **SVM achieved the highest accuracy (89.38%) and is considered the most suitable model for deployment.**

---

## üìà Visualization

- **Confusion Matrices** were plotted for each model to analyze classification errors.  
- **Accuracy Comparison Bar Chart** clearly shows SVM and Logistic Regression outperforming other models.  

![Model Comparison Chart](path_to_your_chart.png)

---

## üìä Deep Learning Models Training & Results  

### üîπ BiLSTM (Bidirectional LSTM)  
- Achieved **Test Accuracy = 88.55%** üéØ  
- Training showed steady improvement with strong convergence.  
- Best performance around **Epoch 3‚Äì4**, after which validation loss slightly increased (early stopping prevented overfitting).  

**Training Snapshot:**  
- Epoch 1 ‚Üí Accuracy: 76.17% | Val Accuracy: 86.44%  
- Epoch 2 ‚Üí Accuracy: 90.30% | Val Accuracy: 88.55%  
- Epoch 3 ‚Üí Accuracy: 93.05% | Val Accuracy: 88.78%  
- Final Test Accuracy ‚Üí **88.55%**  

---

### üîπ Deep CNN for Text  
- Achieved **Test Accuracy = 87.18%** ‚≠ê  
- Learned local n-gram features effectively but slightly less stable on validation data compared to BiLSTM.  
- Peaked around **Epoch 3**, then validation accuracy plateaued.  

**Training Snapshot:**  
- Epoch 1 ‚Üí Accuracy: 67.16% | Val Accuracy: 84.51%  
- Epoch 2 ‚Üí Accuracy: 90.07% | Val Accuracy: 86.07%  
- Epoch 3 ‚Üí Accuracy: 94.19% | Val Accuracy: 87.18%  
- Final Test Accuracy ‚Üí **87.18%**  

---

### üìä Comparison of Deep Models  

| Model       | Test Accuracy | Key Notes                                                                 |
|-------------|--------------|---------------------------------------------------------------------------|
| **BiLSTM**  | **88.55%** üéØ | - Strong contextual understanding <br> - Best performance, less overfitting |
| **CNN**     | 87.18% ‚≠ê    | - Captures local n-gram features <br> - Faster training, slightly lower accuracy |

