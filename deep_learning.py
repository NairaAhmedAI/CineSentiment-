# -*- coding: utf-8 -*-
"""Deep Learning (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KF5xk-jgqgvkoQgqqyLMU4RF-ffPDKZj
"""
# ===============================
# Step 1:Basic Libraries
# ===============================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# ===============================
# Sklearn (for ML + evaluation)
# ===============================
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# ===============================
# Deep Learning (TensorFlow / Keras)
# ===============================
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Embedding, LSTM, Dense, Dropout, Bidirectional,
    Conv1D, MaxPooling1D, GlobalMaxPooling1D
)
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping

# Optimizers
from tensorflow.keras.optimizers import Adam



# ======================================
# Step 2: Prepare the dataset
# (Assume df_models has 'cleaned_review_final' and 'encoded_label')
# ======================================
X = df_models['cleaned_review_final']   # Cleaned text reviews
y = df_models['encoded_label']          # Encoded labels (0 = negative, 1 = positive)

# Train-test split (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Convert labels into numpy arrays
y_train = np.array(y_train)
y_test = np.array(y_test)

# ======================================
# Step 3: Tokenization and Padding
# ======================================
vocab_size = 20000        # Increase vocabulary size for richer representation
max_length = 200          # Longer sequence length for better context
embedding_dim = 128       # Dimension of embedding vectors
# 3.1 Initialize tokenizer
tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")

# 3.2 Fit tokenizer on training data
tokenizer.fit_on_texts(X_train)

# 3.3 Convert text to sequences of integers
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# 3.4 Pad sequences to the same length
X_train_dl = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')
X_test_dl = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')

# ======================================
# Step 4: Build the Improved RNN + BiLSTM Model
# ======================================
from tensorflow.keras.optimizers import Adam
model = Sequential([
    # 4.1 Embedding layer
    Embedding(vocab_size, embedding_dim, input_length=max_length),

    # 4.2 First Bidirectional LSTM layer
    Bidirectional(LSTM(256, return_sequences=True)),
    Dropout(0.4),

    # 4.3 Second Bidirectional LSTM layer
    Bidirectional(LSTM(128)),
    Dropout(0.4),

    # 4.4 Dense hidden layer
    Dense(128, activation='relu'),
    Dropout(0.3),

    # 4.5 Dense hidden layer
    Dense(64, activation='relu'),
    Dropout(0.3),

    # 4.6 Output layer (Binary classification)
    Dense(1, activation='sigmoid')
])

# Print model summary
model.summary()

# ======================================
# Step 5: Compile the Model
# ======================================
optimizer = Adam(learning_rate=1e-4)   # Lower learning rate for better convergence
model.compile(
    loss='binary_crossentropy',
    optimizer=optimizer,
    metrics=['accuracy']
)

# ======================================
# Step 6: Train the Model
# ======================================
early_stop = EarlyStopping(
    monitor='val_loss',           # monitor validation loss
    patience=3,                   # stop if no improvement for 3 epochs
    restore_best_weights=True     # keep the best model
)

history = model.fit(
    X_train_dl, y_train,
    epochs=15,                    # maximum number of epochs
    batch_size=64,                # batch size
    validation_data=(X_test_dl, y_test),
    callbacks=[early_stop],
    verbose=1
)

# ======================================
# Step 7: Evaluate the Model
# ======================================
loss, accuracy = model.evaluate(X_test_dl, y_test, verbose=1)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

# ===============================
# Step 8: Evaluate the model
# ===============================
# Get predictions
y_pred_prob = model.predict(X_test_dl)
y_pred = (y_pred_prob > 0.5).astype(int)

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))

# Confusion matrix visualization
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=['Negative', 'Positive'],
            yticklabels=['Negative', 'Positive'])
plt.title("RNN + LSTM Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ======================================
# Step 4: Build Deep CNN Model
# ======================================
embedding_dim = 128   # embedding dimension to represent words

model_cnn_deep = Sequential([

    # 4.1 Embedding Layer
    # Converts each word index into a dense vector of size `embedding_dim`
    Embedding(vocab_size, embedding_dim, input_length=max_length),

    # 4.2 First Convolutional Block
    # Conv1D learns local n-gram patterns in text
    Conv1D(filters=256, kernel_size=5, activation='relu'),
    MaxPooling1D(pool_size=2),    # downsampling to reduce sequence length
    Dropout(0.3),                 # prevents overfitting

    # 4.3 Second Convolutional Block
    Conv1D(filters=128, kernel_size=5, activation='relu'),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),

    # 4.4 Third Convolutional Block
    Conv1D(filters=64, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),

    # 4.5 Global Pooling Layer
    # Reduces sequence dimension into a single vector (like flattening)
    GlobalMaxPooling1D(),

    # 4.6 Fully Connected Layers
    Dense(256, activation='relu'),
    Dropout(0.4),

    Dense(128, activation='relu'),
    Dropout(0.3),

    Dense(64, activation='relu'),
    Dropout(0.3),

    # 4.7 Output Layer (Binary Classification)
    Dense(1, activation='sigmoid')   # sigmoid for 0/1 prediction
])

# ======================================
# Step 5: Compile CNN Model
# ======================================
model_cnn_deep.compile(
    loss='binary_crossentropy',   # binary classification loss
    optimizer='adam',             # Adam optimizer
    metrics=['accuracy']          # track accuracy during training
)

# Print model summary
model_cnn_deep.summary()

early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True) # monitor validation loss

history_cnn = model_cnn_deep.fit(
    X_train_dl, y_train,
    epochs=15,
    batch_size=64,
    validation_data=(X_test_dl, y_test),
    callbacks=[early_stop],
    verbose=1
)

loss, accuracy =model_cnn_deep.evaluate(X_test_dl, y_test, verbose=1)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

# ===============================
# Step 6: Evaluate the CNN model
# ===============================
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Predictions
y_pred_prob = model_cnn_deep.predict(X_test_dl)
y_pred = (y_pred_prob > 0.5).astype(int)

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))

# Confusion matrix visualization
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Greens",
            xticklabels=['Negative', 'Positive'],
            yticklabels=['Negative', 'Positive'])
plt.title("CNN Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

