# -*- coding: utf-8 -*-
"""Machine Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oUpcqayfQ3UdWTud7MCk-QqKiOVWSwxT
"""

# ===============================
# Step1:Basic Libraries
# ===============================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# ===============================
# Sklearn Tools
# ===============================
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import LabelEncoder

# Feature extraction
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier


# ======================================
# Step 2: Prepare the dataset
# (Assume df_models has 'cleaned_review_final' and 'encoded_label')
# ======================================
X = df_models['cleaned_review_final']   # Cleaned text reviews
# Encoded labels (0 = negative, 1 = positive)
y = df_models['encoded_label']

# Train-test split (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)


"""# ======================================
# Traditional Machine Learning Models for Text Classification
# ======================================
"""

# ======================================
# Logistic Regression (TF-IDF) with Custom Threshold
# ======================================

# Step 1: Text Representation (TF-IDF)
# Convert text into weighted features (term frequency * inverse document frequency)
vectorizer = TfidfVectorizer()
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Step 2: Define Logistic Regression Model
# Logistic Regression is a linear model, often strong for text classification
my_model = LogisticRegression(max_iter=2000)

# Step 3: Train the Model
my_model.fit(X_train_tfidf, y_train)

# Step 4: Predictions (default threshold = 0.5)
y_pred = my_model.predict(X_test_tfidf)

# Step 5: Get prediction probabilities
y_probs = my_model.predict_proba(X_test_tfidf)

# Step 6: Custom Threshold Prediction Function
# This function adjusts classification by changing the threshold (default 0.6)


def custom_predict(probs, threshold=0.6):
    preds = []
    for p in probs:
        prob_neg, prob_pos = p
        if prob_pos >= threshold:
            preds.append("Positive")
        elif prob_neg >= threshold:
            preds.append("Negative")
        else:
            preds.append("Neutral")  # uncertain case
    return preds


y_custom_pred = custom_predict(y_probs, threshold=0.6)

# Step 7: Show random sample predictions
idx = np.random.choice(X_test.shape[0], 20, replace=False)

# Map numeric labels back to original class names
label_map = dict(zip(le.transform(le.classes_), le.classes_))

sample_results = pd.DataFrame({
    "Review": [X_test.iloc[i] for i in idx],
    "Actual": [label_map[y_test[i]] for i in idx],
    "Predicted": [y_custom_pred[i] for i in idx]
})
print("\n=== Sample Predictions with Custom Threshold ===")
print(sample_results)

# Step 8: Model Evaluation
print(f"\n{my_model} Classification Report:\n")
print(classification_report(y_test, y_pred))
acc = accuracy_score(y_test, y_pred) * 100
print(f"Accuracy: {acc:.2f}%")

# Step 9: Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Logistic Regression (TF-IDF) Confusion Matrix')
plt.show()

# Step 1: Text Representation using Bag of Words
# Convert text into numeric features (word counts)
cv = CountVectorizer(stop_words='english')
X_train_cv = cv.fit_transform(X_train)
X_test_cv = cv.transform(X_test)

# Step 2: Naive Bayes with Bag of Words
# Simple probabilistic model, good baseline for text classification
nb = MultinomialNB()
nb.fit(X_train_cv, y_train)
y_pred_nb = nb.predict(X_test_cv)

# Report performance
acc_nb = accuracy_score(y_test, y_pred_nb) * 100
print("\n=== Naive Bayes (Bag of Words) ===")
print("Accuracy:", acc_nb, "%")
print("Classification Report:\n", classification_report(y_test, y_pred_nb))

# Confusion Matrix shows classification errors
cm = confusion_matrix(y_test, y_pred_nb)
plt.figure(figsize=(5, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Oranges",
            xticklabels=nb.classes_, yticklabels=nb.classes_)
plt.title("Random Forest (TF-IDF) Confusion Matrix")
plt.show()

# Step 3: Support Vector Machine with TF-IDF
# SVM usually performs well for text classification with TF-IDF features
svm = LinearSVC()
svm.fit(X_train_tfidf, y_train)
y_pred_svm = svm.predict(X_test_tfidf)

acc_svm = accuracy_score(y_test, y_pred_svm) * 100
print("\n=== Support Vector Machine (TF-IDF) ===")
print("Accuracy:", acc_svm, "%")
print("Classification Report:\n", classification_report(y_test, y_pred_svm))

# Confusion Matrix shows classification errors
cm = confusion_matrix(y_test, y_pred_svm)
plt.figure(figsize=(5, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Purples",
            xticklabels=svm.classes_, yticklabels=svm.classes_)
plt.title("SVM (TF-IDF) Confusion Matrix")
plt.show()

# Step 4: Random Forest with TF-IDF
# Ensemble model using multiple decision trees
rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)
rf.fit(X_train_tfidf, y_train)
y_pred_rf = rf.predict(X_test_tfidf)

acc_rf = accuracy_score(y_test, y_pred_rf) * 100
print("\n=== Random Forest (TF-IDF) ===")
print("Accuracy:", acc_rf, "%")
print("Classification Report:\n", classification_report(y_test, y_pred_rf))

cm = confusion_matrix(y_test, y_pred_rf)
plt.figure(figsize=(5, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Reds",
            xticklabels=rf.classes_, yticklabels=rf.classes_)
plt.title("Random Forest (TF-IDF) Confusion Matrix")
plt.show()

# ======================================
# Step X: Model Accuracy Comparison (ML + DL Models)
# ======================================

# Classical ML models
acc_nb = accuracy_score(y_test, y_pred_nb) * 100        # Naive Bayes
acc_svm = accuracy_score(y_test, y_pred_svm) * 100      # SVM
acc_rf = accuracy_score(y_test, y_pred_rf) * 100        # Random Forest
acc_lr = accuracy_score(y_test, y_pred) * 100        # Logistic Regression


# Combine all models
models = [
    'Naive Bayes',
    'SVM',
    'Random Forest',
    'Logistic Regression'
]
accuracies = [acc_nb, acc_svm, acc_rf, acc_lr]

# Plot comparison
plt.figure(figsize=(12, 6))
bars = plt.bar(models, accuracies, color=[
               'violet', 'salmon', 'gold', 'lightblue'])
plt.ylim(0, 100)
plt.ylabel('Accuracy (%)')
plt.title('Model Comparison: Accuracy (ML)')

# Add accuracy values above bars
for bar, acc in zip(bars, accuracies):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
             f"{acc:.2f}%", ha='center', fontsize=11, fontweight='bold')

plt.show()
