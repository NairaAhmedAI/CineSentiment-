# -*- coding: utf-8 -*-
"""Machine Learning (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_8UADzP18gln3ZTd-c9JrGGdkd88mATh
"""
# Basic
import pandas as pd
import numpy as np
import re

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Sklearn
from sklearn.preprocessing import LabelEncoder

# NLTK
import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer


df = pd.read_csv(r"C:\Users\DELL\Downloads\projecr nlp\IMDB Dataset.csv")


def to_lowercase(text: str) -> str:
    """
    Convert the entire text to lowercase.
    """
    return text.lower()


# Apply function to the DataFrame column
df['lowercase_review'] = df['review'].apply(to_lowercase)
df.head()

le = LabelEncoder()
df["encoded_label"] = le.fit_transform(df["sentiment"])


print(dict(zip(le.classes_, le.transform(le.classes_))))

df.head()


def remove_emojis_and_hashtags(text):
    """
    Remove emojis and hashtags from text
    """
    # Regex for emojis
    emoji_pattern = re.compile(
        "["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags
        "]+", flags=re.UNICODE
    )
    # 1. Remove emojis
    text = emoji_pattern.sub(r'', text)
    # 2. Remove hashtags
    text = re.sub(r'#\w+', '', text)
    text = re.sub(r'<br\s*/?>', ' ', text)
    return text


df['cleaned_review'] = df['lowercase_review'].apply(remove_emojis_and_hashtags)

nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')


stop_words = set(stopwords.words('english'))


def clean_text_basic(text):
    """
    2. Remove punctuation
    3. Tokenize
    4. Remove stopwords except negation words
    """
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', ' ', text)
    tokens = word_tokenize(text)
    tokens = [t.lower() for t in tokens if t.lower() not in stop_words]
    return tokens


df['tokens'] = df['cleaned_review'].apply(clean_text_basic)
df.head()

nltk.download('averaged_perceptron_tagger')
nltk.download('averaged_perceptron_tagger_eng')

lemmatizer = WordNetLemmatizer()

# Penn Treebank â†’ WordNet


def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN  # default noun


def apply_lemmatization(tokens):
    pos_tags = nltk.pos_tag(tokens)  # [(word, tag), ...]
    return [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]


df['tokens_lemmatized'] = df['tokens'].apply(apply_lemmatization)
df['cleaned_review_final'] = df['tokens_lemmatized'].apply(
    lambda x: ' '.join(x))

df.sample(10)

df_models = df[['cleaned_review_final', 'encoded_label']].copy()
print(df_models.head())
